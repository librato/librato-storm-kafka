#!/usr/bin/env ruby

require 'trollop'
require 'zookeeper'
require 'yajl/json_gem'
require 'faraday_middleware'
require 'kafka'
require 'excon'
require 'multi_xml'
require 'aws-sdk'

require 'librato/metrics'

$:.unshift File.join(File.dirname(__FILE__), '../lib')

require 'librato-storm-kafka'

# force nokogiri
MultiXml.parser = :nokogiri

parser = Trollop::Parser.new do
  version "librato-storm-kafka version %s" %
    [Librato::Storm::Kafka::VERSION]

  opt :api, "Change the API endpoint", {
    :type => :string, :default => "metrics-api.librato.com"
  }

  opt :email, "Librato Metrics Email", {
    :type => :string, :default => ENV['LIBRATO_METRICS_EMAIL']
  }

  opt :token, "Librato Metrics API Token", {
    :type => :string, :default => ENV['LIBRATO_METRICS_TOKEN']
  }

  opt :prefix, "Metric name prefix", {
    :type => :string, :default => "kafkastorm."
  }

  opt :floor_in_secs, "floor() measure times to this interval", {
    :type => :int
  }

  opt :environment, "environment name to be used for metric sources", {
    :type => :string, :default => "unknown"
  }

  opt :mx4j_port, "Port to connect to MX4J on", {
    :default => 8082
  }

  opt :threads, "How many polling threads to start", {
    :default => 4
  }

  opt :zk_servers, "ZooKeeper servers (comma separated)", {
    :type => :string
  }

  opt :zk_port, "Port to connect to ZK on", {
    :default => 2181
  }

  opt :zk_prefix, "Prefix path in ZK to find consumers", {
    :default => "/kafkastorm"
  }

  opt :zk_brokers, "Path to locate brokers in ZK", {
    :default => "/brokers"
  }

  opt :create_aws_custom_metrics, "Whether or not AWS custom metrics should also be submitted", {
    :type => :boolean, :default => false
  }

  opt :aws_access_key_id, "AWS access key id for custom metrics", {
    :type => :string, :default => ENV['AWS_ACCESS_KEY_ID']
  }

  opt :aws_secret_access_key, "AWS secret access key for custom metrics", {
    :type => :string, :default => ENV['AWS_SECRET_ACCESS_KEY']
  }

  opt :aws_namespace, "AWS namespace for the custom metric", {
    :type => :string, :default => "storm-kafka"
  }

  opt :breakout_aws_custom_metrics, "Whether or not AWS custom metrics should be given a source dimension of host/parition number", {
    :type => :boolean, :default => false
  }

  opt :honeybadger_api, "Honeybadger API key", {
    :type => :string, :default => ENV['HONEYBADGER_API_KEY']
  }

  opt :debug, "Enable debug logging", {
    :type => :boolean, :default => false
  }
end

opts = Trollop::with_standard_exception_handling(parser) do
  if ARGV.length == 0
    raise Trollop::HelpNeeded
  end

  opts = parser.parse ARGV
  %w{ email token zk_servers }.each do |f|
    unless opts[f.to_sym]
      $stderr.puts "Error: Must specify option --%s." % [f]
      puts
      raise Trollop::HelpNeeded
    end
  end
  opts
end

$start_time = Time.now

$mt = Time.now.tv_sec
if opts[:floor_in_secs]
  $mt = ($mt / opts[:floor_in_secs]) * opts[:floor_in_secs]
end

$parts = []
$threads = []
$completed = []
$mutex = Mutex.new
$honeybadger_api = nil

if opts[:honeybadger_api].to_s.length > 0
  ENV['HONEYBADGER_USER_INFORMER_ENABLED'] = 'false'
  ENV['HONEYBADGER_FEEDBACK_ENABLED'] = 'false'

  ENV['HONEYBADGER_METRICS_ENABLED'] = 'false'
  ENV['HONEYBADGER_METRICS_GC_PROFILER'] = 'false'

  ENV['HONEYBADGER_TRACES_ENABLED'] = 'false'

  require 'honeybadger'

  honeybadger_config = Honeybadger::Config.new()
  Honeybadger.start(honeybadger_config)

  $honeybadger_api = opts[:honeybadger_api]
end

def honeybadger_notify(ex)
  if $honeybadger_api
    Honeybadger.notify(ex)
  end
end

endpoint = "https://#{opts[:api]}"
$client = Librato::Metrics::Client.new
$client.api_endpoint = endpoint
$client.authenticate opts[:email], opts[:token]

zk_connect_str = opts[:zk_servers].split(",").
  map {|s| "#{s}:#{opts[:zk_port]}"}.join(",")

$z = Zookeeper.new(zk_connect_str)

$enable_logging = opts.debug
def debug_log(msg)
  return unless $enable_logging
  $mutex.lock
  puts msg
  $mutex.unlock
end

def log(msg)
  $mutex.lock
  puts msg
  $mutex.unlock
end

# Returns time of block in seconds
def time_block
  startt = Time.now
  yield
  endt = Time.now
  endt - startt
end

def submit(q)
  begin
    q.submit
    q.clear
  rescue => err
    $stderr.puts "Failed to submit stats to Librato Metrics: %s" %
      [err.message]
    exit 1
  end
end

def with_children(path, &blk)
  kids = $z.get_children(:path => path)
  if kids[:rc] != 0
    $stderr.puts "Unable to list children at path: #{path}"
    exit 1
  end
  kids[:children].each {|k| yield(k, "#{path}/#{k}") }
end

def http(host, path)
  uri = URI.parse("http://#{host}:8082/#{path}") rescue nil
  unless uri
    puts "Unable to parse host '#{host}' and path '#{path}'"
    return nil
  end

  Excon.new(uri.to_s, :connect_timeout => 3)
end

def bean_lookup(host, beanname)
  begin
    conn = http(host, "/mbean")
    unless conn
      return nil
    end

    resp = conn.get(:query => {:objectname => beanname, :template  => 'identity' },
                    :read_timeout => 5)
    body = resp.body
    unless body.include?("<MBean")
      return nil
    end
    MultiXml.parse(body)
  rescue Excon::Errors::SocketError => e
    log "Socket error when connecting to #{host}: #{e.inspect}"
    raise e
  rescue Excon::Errors::Timeout => e
    log "Could not get response from bean lookup from #{host}: #{e.inspect}"
    raise Errno::ETIMEDOUT
  end
end

def partition_bean_lookup(host, partname, partidx)
  bean_lookup(host, "kafka:type=kafka.logs.#{partname}-#{partidx}")
end

def jmxinfo_to_attrs(jmxinfo)
  attrs = {}
  jmxinfo['MBean']['Attribute'].each do |attr|
    next if attr['isnull'] != 'false'
    if ['long', 'int'].include?(attr['type'])
      attrs[attr['name']] = Integer(attr['value'])
    elsif attr['type'] =~ /CompositeData/
      # XXX: Hack out the contents of the composite type. This should
      # really be expanded in the XML
      fields = attr['value'].gsub(/^.*,contents={/, '').gsub(/}\)$/, '')
        .gsub(/[=,]/, '@').split("@")
      attrs[attr['name']] = Hash[*fields.map {|u| u.strip}]
    else
      attrs[attr['name']] = attr['value']
    end
  end
  attrs
end

def do_thr()
  ret = true
  while true
    $mutex.lock
    work = $parts.pop
    $mutex.unlock
    break if work.nil?

    begin
      result = monitor_partition(work)
    rescue Errno::ETIMEDOUT, Errno::EHOSTUNREACH => e
      log "Could not monitor partition: #{e.inspect}"
    end

    unless result
      log "Failed to monitor partition: #{work[:part]}"
      next
    end

    $mutex.lock
    $completed << result
    $mutex.unlock
  end
  ret
rescue Exception => e
  honeybadger_notify(e)
  raise e
end

def add_monitor_partition(opts, part, pfx = "")
  work = {}
  metric_prefix = "#{pfx}#{opts[:prefix]}topics.#{part['topology']['name']}.#{part['topic']}"
  source = "#{part['broker']['host']}_#{part['partition']}"
  debug_log "Adding monitor partition metric_prefix=#{metric_prefix} source=#{source}"
  q = $client.new_queue({ :prefix => metric_prefix, :source => source, :skip_measurement_times => true})

  work = {
    :queue => q,
    :custom_metric_prefix => metric_prefix,
    :custom_metric_source => source,
    :opts => opts,
    :part => part,
    :pfx => pfx
  }

  $mutex.lock
  $parts << work
  $mutex.unlock
end

def monitor_partition(work)
  q = work[:queue]
  opts = work[:opts]
  part = work[:part]
  pfx = work[:pfx]
  custom_metrics = []
  custom_metric_prefix = work[:custom_metric_prefix]
  custom_metric_source = work[:custom_metric_source]

  host = part['broker']['host']

  jmxinfo = nil
  jmx_time = time_block do
    jmxinfo = partition_bean_lookup(host,
                                    part['topic'], part['partition'])
  end

  unless jmxinfo
    return nil
  end

  attrs = jmxinfo_to_attrs(jmxinfo)

  endoffset = nil
  kafka_time = time_block do
    consumer = Kafka::Consumer.new({ :host => host,
                                     :topic => part['topic'],
                                     :partition => part['partition']})
    endoffset = consumer.fetch_latest_offset
    consumer.disconnect
  end

  q.add :bytes_vol => {:value => endoffset, :type => :counter}

  diff = endoffset > part['offset'] ? endoffset - part['offset'] : 0
  q.add(:pending_bytes => { :value => diff })

  if opts.create_aws_custom_metrics
    custom_metrics << {
      :prefix => custom_metric_prefix,
      :source => custom_metric_source,
      :name => "pending_bytes",
      :value => diff
    }
  end

  if attrs['NumAppendedMessages'] > 0
    q.add :msg_vol => {:value => attrs['NumAppendedMessages'], :type => :counter}
  end

  {:queue => q, :custom_metrics => custom_metrics, :host => host, :jmx_time => jmx_time, :kafka_time => kafka_time}
end

def get_active_brokers(opts)
  brokers = {}

  with_children("#{opts[:zk_brokers]}/ids") do |id, fullid|
    brokerinfo = $z.get(:path => fullid)
    if brokerinfo[:rc] != 0
      $stderr.puts "Failed to lookup broker id: #{fullid}"
      exit 1
    end

    # eg: 10.151.5.190-1421179184361:10.151.5.190:9092
    sp = brokerinfo[:data].split(":")
    brokstr = "%s:%s" % [sp[1], sp[2]]
    brokers[brokstr] = true
  end

  brokers
end


def main(opts)
  brokers = get_active_brokers(opts)
  #puts "brokers: #{brokers.inspect}"

  partitions = 0
  submit_queue = $client.new_queue(:measure_time => $mt, :autosubmit_count => 300, :skip_measurement_times => true)
  with_children(opts[:zk_prefix]) do |spout, fullsp|
    with_children(fullsp) do |part, fullpart|
      next if part == 'user'

      partinfo = $z.get(:path => fullpart)
      if partinfo[:rc] != 0
        $stderr.puts "Failed to lookup partition: #{fullpart}"
        exit 1
      end

      body = JSON.parse(partinfo[:data])
      tmppfx = ""

      brokstr = "%s:%d" % [body['broker']['host'], body['broker']['port']]
      next unless brokers[brokstr]

      add_monitor_partition(opts, body, tmppfx)
      partitions += 1
    end
  end

  # Start our threads to poll in parallel
  1.upto(opts[:threads]) do
    $threads << Thread.new do
      do_thr
    end
  end

  # Wait for threads to exit
  $threads.each do |thr|
    if !thr.value
      $stderr.puts "Thread #{thr.inspect} returned failure!"
      exit 1
    end
  end

  times_aggregator = Librato::Metrics::Aggregator.new(:client => $client, :prefix => opts[:prefix].gsub(/\.$/, ""), :measure_time => $mt)
  hosts = {}
  $custom_metrics = []
  $completed.each do |comp|
    submit_queue.merge!(comp[:queue])
    $custom_metrics << comp[:custom_metrics]
    hosts[comp[:host]] = 1

    times_aggregator.add 'jmx_time' => {value: comp[:jmx_time], source: comp[:host]} if comp[:jmx_time]
    times_aggregator.add 'kafka_time' => {value: comp[:kafka_time], source: comp[:host]}  if comp[:kafka_time]

    if submit_queue.size >= 300
      submit(submit_queue)
    end
  end

  if opts.create_aws_custom_metrics && !$custom_metrics.empty?
    cw = Aws::CloudWatch::Client.new(:access_key_id => opts.aws_access_key_id, :secret_access_key => opts.aws_secret_access_key, :region => "us-east-1")
    metric_data = $custom_metrics.flatten.compact.inject([]) do |arr, metric|
      data = {
        :metric_name => "#{metric[:prefix]}.#{metric[:name]}",
        :value => metric[:value]
      }
      if opts.breakout_aws_custom_metrics
        data[:dimensions] = [ {:name => "source", :value => metric[:source]} ]
      end
      arr << data
      arr
    end
    # break into groups of 20 things
    metric_data.each_slice(20) do |data|
      if !data.empty?
        debug_log "Submitting #{data.size} custom metrics"
        t = time_block do
          cw.put_metric_data({ :namespace => opts.aws_namespace, :metric_data => data })
        end
        times_aggregator.add 'cloudwatch_time' => {value: t}
      end
    end
  end

  submit_queue.add "#{opts[:prefix]}active_partitions" => {:value => partitions, :source => opts[:environment]}

  # Now check host-level options
  active_hosts = 0
  hosts.each_pair do |host, v|
    q = $client.new_queue({ :prefix => "#{opts[:prefix]}hosts",
                            :source => "#{host}", :skip_measurement_times => true})
    begin
      jmxinfo = nil
      t = time_block do
        jmxinfo = bean_lookup(host, "kafka:type=kafka.BrokerAllTopicStat")
      end
      times_aggregator.add 'jmx_time' => {value: t, source: host}

      next unless jmxinfo

      attrs = jmxinfo_to_attrs(jmxinfo)
      q.add :bytes_in => {:value => attrs['BytesIn'], :type => :counter}
      q.add :bytes_out => {:value => attrs['BytesOut'], :type => :counter}
      q.add :failed_fetch_request => {:value => attrs['FailedFetchRequest'], :type => :counter}
      q.add :failed_produce_request => {:value => attrs['FailedProduceRequest'], :type => :counter}
      q.add :messages_in => {:value => attrs['MessagesIn'], :type => :counter}

      jmxinfo = nil
      t = time_block do
        jmxinfo = bean_lookup(host, "kafka:type=kafka.LogFlushStats")
      end
      times_aggregator.add 'jmx_time' => {value: t, source: host}

      if jmxinfo
        attrs = jmxinfo_to_attrs(jmxinfo)
        q.add :num_flushes => {:value => attrs['NumFlushes'], :type => :counter}
        q.add :total_flush_ms => {:value => attrs['TotalFlushMs'], :type => :counter}
        q.add :max_flush_ms => {:value => attrs['MaxFlushMs'], :type => :gauge}
      end

      ["Copy", "PS Scavenge", "PS MarkSweep"].each do |collector|
        jmxinfo = nil
        t = time_block do
          jmxinfo = bean_lookup(host, "java.lang:type=GarbageCollector,name=#{collector}")
        end
        times_aggregator.add 'jmx_time' => {value: t, source: host}

        next unless jmxinfo
        gcname = collector.gsub(" ", "_")
        attrs = jmxinfo_to_attrs(jmxinfo)
        q.add :"gc.#{gcname}.collection.count" => {:value => attrs['CollectionCount'], :type => :counter}
        q.add :"gc.#{gcname}.collection.time" => {:value => attrs['CollectionTime'], :type => :counter}
      end

      jmxinfo = nil
      t = time_block do
        jmxinfo = bean_lookup(host, "java.lang:type=Memory")
      end
      times_aggregator.add 'jmx_time' => {value: t, source: host}

      attrs = jmxinfo_to_attrs(jmxinfo)
      attrs.each_pair do |name, values|
        next unless name =~ /HeapMemory/
        values.each_pair do |attrn, attrv|
          q.add "memory.#{name.downcase}.#{attrn.downcase}" => {:value => Integer(attrv)}
        end
      end

      active_hosts += 1
    rescue Errno::ETIMEDOUT, Errno::EHOSTUNREACH
      next
    end
    submit_queue.merge!(q)
  end

  submit_queue.add "#{opts[:prefix]}active_hosts" => {:value => active_hosts, :source => opts[:environment]}

  submit(submit_queue)
  times_aggregator.submit

  $end_time = Time.now
  submit_queue.add "#{opts[:prefix]}run_time" => {:value => ($end_time - $start_time), :source => opts[:environment]}
  submit(submit_queue)
end

begin
  main(opts)
rescue Exception => e
  honeybadger_notify(e)
  raise e
end

# Local Variables:
# mode: ruby
# End:
