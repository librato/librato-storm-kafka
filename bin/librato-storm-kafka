#!/usr/bin/env ruby

require 'trollop'
require 'zookeeper'
require 'yajl/json_gem'
require 'faraday_middleware'
require 'kafka'
require 'excon'
require 'multi_xml'
require 'aws'

require 'librato/metrics'

$:.unshift File.join(File.dirname(__FILE__), '../lib')

require 'librato-storm-kafka'

# force nokogiri
MultiXml.parser = :nokogiri

parser = Trollop::Parser.new do
  version "librato-storm-kafka version %s" %
    [Librato::Storm::Kafka::VERSION]

  opt :api, "Change the API endpoint", {
    :type => :string, :default => "metrics-api.librato.com"
  }

  opt :email, "Librato Metrics Email", {
    :type => :string, :default => ENV['LIBRATO_METRICS_EMAIL']
  }

  opt :token, "Librato Metrics API Token", {
    :type => :string, :default => ENV['LIBRATO_METRICS_TOKEN']
  }

  opt :prefix, "Metric name prefix", {
    :type => :string, :default => "kafkastorm."
  }

  opt :floor_in_secs, "floor() measure times to this interval", {
    :type => :int
  }

  opt :mx4j_port, "Port to connect to MX4J on", {
    :default => 8082
  }

  opt :threads, "How many polling threads to start", {
    :default => 4
  }

  opt :zk_servers, "ZooKeeper servers (comma separated)", {
    :type => :string
  }

  opt :zk_port, "Port to connect to ZK on", {
    :default => 2181
  }

  opt :zk_prefix, "Prefix path in ZK to find consumers", {
    :default => "/kafkastorm"
  }

  opt :create_aws_custom_metrics, "Whether or not AWS custom metrics should also be submitted", {
    :type => :boolean, :default => false
  }

  opt :aws_access_key_id, "AWS access key id for custom metrics", {
    :type => :string, :default => ENV['AWS_ACCESS_KEY_ID']
  }

  opt :aws_secret_access_key, "AWS secret access key for custom metrics", {
    :type => :string, :default => ENV['AWS_SECRET_ACCESS_KEY']
  }

  opt :aws_namespace, "AWS namespace for the custom metric", {
    :type => :string, :default => "storm-kafka"
  }

  opt :breakout_aws_custom_metrics, "Whether or not AWS custom metrics should be given a source dimension of host/parition number", {
    :type => :boolean, :default => false
  }

  opt :debug, "Enable debug logging", {
    :type => :boolean, :default => false
  }
end

opts = Trollop::with_standard_exception_handling(parser) do
  if ARGV.length == 0
    raise Trollop::HelpNeeded
  end

  opts = parser.parse ARGV
  %w{ email token zk_servers }.each do |f|
    unless opts[f.to_sym]
      $stderr.puts "Error: Must specify option --%s." % [f]
      puts
      raise Trollop::HelpNeeded
    end
  end
  opts
end

$start_time = Time.now

$mt = Time.now.tv_sec
if opts[:floor_in_secs]
  $mt = ($mt / opts[:floor_in_secs]) * opts[:floor_in_secs]
end

$parts = []
$threads = []
$completed = []
$mutex = Mutex.new

endpoint = "https://#{opts[:api]}"
$client = Librato::Metrics::Client.new
$client.api_endpoint = endpoint
$client.authenticate opts[:email], opts[:token]

zk_connect_str = opts[:zk_servers].split(",").
  map {|s| "#{s}:#{opts[:zk_port]}"}.join(",")

$z = Zookeeper.new(zk_connect_str)

$enable_logging = opts.debug
def log(msg)
  return unless $enable_logging
  $mutex.lock
  puts msg
  $mutex.unlock
end

# Returns time of block in seconds
def time_block
  startt = Time.now
  yield
  endt = Time.now
  endt - startt
end

def submit(q)
  begin
    q.submit
    q.clear
  rescue => err
    $stderr.puts "Failed to submit stats to Librato Metrics: %s" %
      [err.message]
    exit 1
  end
end

def with_children(path, &blk)
  kids = $z.get_children(:path => path)
  if kids[:rc] != 0
    $stderr.puts "Unable to list children at path: #{path}"
    exit 1
  end
  kids[:children].each {|k| yield(k, "#{path}/#{k}") }
end

def http(host, path)
  Excon.new(URI.parse("http://#{host}:8082/#{path}").to_s,
            :connect_timeout => 3)
end

def bean_lookup(host, beanname)
  begin
    conn = http(host, "/mbean")
    resp = conn.get(:query => {:objectname => beanname, :template  => 'identity' },
                    :read_timeout => 5)
    body = resp.body
    unless body.include?("<MBean")
      return nil
    end
    MultiXml.parse(body)
  rescue Excon::Errors::SocketError => e
    log "Socket error when connecting to #{host}: #{e.inspect}"
    raise e
  rescue Excon::Errors::Timeout => e
    log "Could not get response from bean lookup from #{host}: #{e.inspect}"
    raise Errno::ETIMEDOUT
  end
end

def partition_bean_lookup(host, partname, partidx)
  bean_lookup(host, "kafka:type=kafka.logs.#{partname}-#{partidx}")
end

def jmxinfo_to_attrs(jmxinfo)
  attrs = {}
  jmxinfo['MBean']['Attribute'].each do |attr|
    next if attr['isnull'] != 'false'
    if ['long', 'int'].include?(attr['type'])
      attrs[attr['name']] = Integer(attr['value'])
    elsif attr['type'] =~ /CompositeData/
      # XXX: Hack out the contents of the composite type. This should
      # really be expanded in the XML
      fields = attr['value'].gsub(/^.*,contents={/, '').gsub(/}\)$/, '')
        .gsub(/[=,]/, '@').split("@")
      attrs[attr['name']] = Hash[*fields.map {|u| u.strip}]
    else
      attrs[attr['name']] = attr['value']
    end
  end
  attrs
end

def do_thr()
  ret = true
  while true
    $mutex.lock
    work = $parts.pop
    $mutex.unlock
    break if work.nil?

    begin
      work = monitor_partition(work)
    rescue Errno::ETIMEDOUT, Errno::EHOSTUNREACH => e
      log "Could not monitor partition: #{e.inspect}"
    end

    $mutex.lock
    $completed << work
    $mutex.unlock
  end
  ret
end

def add_monitor_partition(opts, part, pfx = "")
  work = {}
  metric_prefix = "#{pfx}#{opts[:prefix]}topics.#{part['topology']['name']}.#{part['topic']}"
  source = "#{part['broker']['host']}_#{part['partition']}"
  log "Adding monitor partition metric_prefix=#{metric_prefix} source=#{source}"
  q = $client.new_queue({ :prefix => metric_prefix, :source => source, :skip_measurement_times => true})

  work = {
    :queue => q,
    :custom_metric_prefix => metric_prefix,
    :custom_metric_source => source,
    :opts => opts,
    :part => part,
    :pfx => pfx
  }

  $mutex.lock
  $parts << work
  $mutex.unlock
end

def monitor_partition(work)
  q = work[:queue]
  opts = work[:opts]
  part = work[:part]
  pfx = work[:pfx]
  custom_metrics = []
  custom_metric_prefix = work[:custom_metric_prefix]
  custom_metric_source = work[:custom_metric_source]

  host = part['broker']['host']

  jmxinfo = nil
  jmx_time = time_block do
    jmxinfo = partition_bean_lookup(host,
                                    part['topic'], part['partition'])
  end

  attrs = jmxinfo_to_attrs(jmxinfo)

  endoffset = nil
  kafka_time = time_block do
    consumer = Kafka::Consumer.new({ :host => host,
                                     :topic => part['topic'],
                                     :partition => part['partition']})
    endoffset = consumer.fetch_latest_offset
    consumer.disconnect
  end

  q.add :bytes_vol => {:value => endoffset, :type => :counter}

  diff = endoffset > part['offset'] ? endoffset - part['offset'] : 0
  q.add(:pending_bytes => { :value => diff })

  if opts.create_aws_custom_metrics
    custom_metrics << {
      :prefix => custom_metric_prefix,
      :source => custom_metric_source,
      :name => "pending_bytes",
      :value => diff
    }
  end

  if attrs['NumAppendedMessages'] > 0
    q.add :msg_vol => {:value => attrs['NumAppendedMessages'], :type => :counter}
  end

  {:queue => q, :custom_metrics => custom_metrics, :host => host, :jmx_time => jmx_time, :kafka_time => kafka_time}
end

partitions = 0
submit_queue = $client.new_queue(:measure_time => $mt, :autosubmit_count => 300, :skip_measurement_times => true)
with_children(opts[:zk_prefix]) do |spout, fullsp|
  with_children(fullsp) do |part, fullpart|
    next if part == 'user'

    partinfo = $z.get(:path => fullpart)
    if partinfo[:rc] != 0
      $stderr.puts "Failed to lookup partition: #{fullpart}"
      exit 1
    end

    body = JSON.parse(partinfo[:data])
    tmppfx = ""

    add_monitor_partition(opts, body, tmppfx)
    partitions += 1
  end
end

# Start our threads to poll in parallel
1.upto(opts[:threads]) do
  $threads << Thread.new do
    do_thr
  end
end

# Wait for threads to exit
$threads.each do |thr|
  if !thr.value
    $stderr.puts "Thread #{thr.inspect} returned failure!"
    exit 1
  end
end

times_aggregator = Librato::Metrics::Aggregator.new(:client => $client, :prefix => opts[:prefix].gsub(/\.$/, ""), :measure_time => $mt)
hosts = {}
$custom_metrics = []
$completed.each do |comp|
  submit_queue.merge!(comp[:queue])
  $custom_metrics << comp[:custom_metrics]
  hosts[comp[:host]] = 1

  times_aggregator.add 'jmx_time' => {value: comp[:jmx_time], source: comp[:host]} if comp[:jmx_time]
  times_aggregator.add 'kafka_time' => {value: comp[:kafka_time], source: comp[:host]}  if comp[:kafka_time]

  if submit_queue.size >= 300
    submit(submit_queue)
  end
end

if opts.create_aws_custom_metrics && !$custom_metrics.empty?
  cw = AWS::CloudWatch.new(:access_key_id => opts.aws_access_key_id, :secret_access_key => opts.aws_secret_access_key).client
  metric_data = $custom_metrics.flatten.compact.inject([]) do |arr, metric|
    data = {
      :metric_name => "#{metric[:prefix]}.#{metric[:name]}",
      :value => metric[:value]
    }
    if opts.breakout_aws_custom_metrics
      data[:dimensions] = [ {:name => "source", :value => metric[:source]} ]
    end
    arr << data
    arr
  end
  # break into groups of 20 things
  metric_data.each_slice(20) do |data|
    if !data.empty?
      log "Submitting #{data.size} custom metrics"
      t = time_block do
        cw.put_metric_data({ :namespace => opts.aws_namespace, :metric_data => data })
      end
      times_aggregator.add 'cloudwatch_time' => {value: t}
    end
  end
end

submit_queue.add "#{opts[:prefix]}active_partitions" => {:value => partitions}

# Now check host-level options
active_hosts = 0
hosts.each_pair do |host, v|
  q = $client.new_queue({ :prefix => "#{opts[:prefix]}hosts",
                          :source => "#{host}", :skip_measurement_times => true})
  begin
    jmxinfo = nil
    t = time_block do
      jmxinfo = bean_lookup(host, "kafka:type=kafka.BrokerAllTopicStat")
    end
    times_aggregator.add 'jmx_time' => {value: t, source: host}

    attrs = jmxinfo_to_attrs(jmxinfo)
    q.add :bytes_in => {:value => attrs['BytesIn'], :type => :counter}
    q.add :bytes_out => {:value => attrs['BytesOut'], :type => :counter}
    q.add :failed_fetch_request => {:value => attrs['FailedFetchRequest'], :type => :counter}
    q.add :failed_produce_request => {:value => attrs['FailedProduceRequest'], :type => :counter}
    q.add :messages_in => {:value => attrs['MessagesIn'], :type => :counter}

    jmxinfo = nil
    t = time_block do
      jmxinfo = bean_lookup(host, "kafka:type=kafka.LogFlushStats")
    end
    times_aggregator.add 'jmx_time' => {value: t, source: host}

    if jmxinfo
      attrs = jmxinfo_to_attrs(jmxinfo)
      q.add :num_flushes => {:value => attrs['NumFlushes'], :type => :counter}
      q.add :total_flush_ms => {:value => attrs['TotalFlushMs'], :type => :counter}
      q.add :max_flush_ms => {:value => attrs['MaxFlushMs'], :type => :gauge}
    end

    ["Copy", "PS Scavenge", "PS MarkSweep"].each do |collector|
      jmxinfo = nil
      t = time_block do
        jmxinfo = bean_lookup(host, "java.lang:type=GarbageCollector,name=#{collector}")
      end
      times_aggregator.add 'jmx_time' => {value: t, source: host}

      next unless jmxinfo
      gcname = collector.gsub(" ", "_")
      attrs = jmxinfo_to_attrs(jmxinfo)
      q.add :"gc.#{gcname}.collection.count" => {:value => attrs['CollectionCount'], :type => :counter}
      q.add :"gc.#{gcname}.collection.time" => {:value => attrs['CollectionTime'], :type => :counter}
    end

    jmxinfo = nil
    t = time_block do
      jmxinfo = bean_lookup(host, "java.lang:type=Memory")
    end
    times_aggregator.add 'jmx_time' => {value: t, source: host}

    attrs = jmxinfo_to_attrs(jmxinfo)
    attrs.each_pair do |name, values|
      next unless name =~ /HeapMemory/
      values.each_pair do |attrn, attrv|
        q.add "memory.#{name.downcase}.#{attrn.downcase}" => {:value => Integer(attrv)}
      end
    end

    active_hosts += 1
  rescue Errno::ETIMEDOUT, Errno::EHOSTUNREACH
    next
  end
  submit_queue.merge!(q)
end

submit_queue.add "#{opts[:prefix]}active_hosts" => {:value => active_hosts}

$end_time = Time.now

submit_queue.add "#{opts[:prefix]}run_time" => {:value => ($end_time - $start_time)}

submit(submit_queue)
times_aggregator.submit

# Local Variables:
# mode: ruby
# End:
